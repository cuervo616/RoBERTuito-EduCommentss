{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkHrU7a18bCN"
      },
      "source": [
        "# Entrenamiento K-Folds con parada temprana\n",
        "\n",
        "Autores:\n",
        "- Pablo Quito\n",
        "- Juan Valdiviezo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk6NLgLj8WzL",
        "outputId": "be0dc3a6-79be-48e6-fd79-e9c3a6c172c9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki9Haluf8hlN"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "from textwrap import wrap\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import RobertaModel, AutoModel\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PkgGY3Y8vyd",
        "outputId": "80141ae5-f255-4d65-a63c-afadf5a8cdb6"
      },
      "outputs": [],
      "source": [
        "# Inicializaci√≥n\n",
        "ROUTE = '/content/drive/My Drive/Intelektubies/Datos/Entrenamiento V5'\n",
        "FILE_NAME = 'df_entrenamiento_v5.xlsx'\n",
        "RANDOM_SEED = 25\n",
        "MAX_LEN = 130\n",
        "BATCH_SIZE = 16  #Anterior 8\n",
        "N_SPLITS = 5  # K-Folds\n",
        "DATASET_PATH = ROUTE + '/' + FILE_NAME\n",
        "NCLASES = 4  # Positivo, Negativo, Neutro, Alerta\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Fuente de datos: \" + DATASET_PATH)\n",
        "print(\"Dispositivo: \" + str(device))\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "OViKYibE_xBi",
        "outputId": "455d4102-0d7a-4dc3-e381-4a09a417cc67"
      },
      "outputs": [],
      "source": [
        "# Cargar datos\n",
        "df = pd.read_excel(DATASET_PATH)\n",
        "print(df.shape)\n",
        "print(\"\\n\".join(wrap(df['comentario'][666])))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "C34YQb7xEmNV",
        "outputId": "188de0de-6f06-4742-d692-465071d3be75"
      },
      "outputs": [],
      "source": [
        "#Mapeo a las categorias para el modelo\n",
        "df['sentimiento'] = df['sentimiento'].map({'Positiva': 2, 'Negativa': 0, 'Neutral': 1,'Alerta': 3}).astype(int)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6vLulF_FJ7Y",
        "outputId": "63843cde-4f31-450c-eefe-0914e8eb04c4"
      },
      "outputs": [],
      "source": [
        "# TOKENIZACI√ìN\n",
        "PRE_TRAINED_MODEL = 'pysentimiento/robertuito-base-uncased-emotion'\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)  # Usa AutoTokenizer para elegir el tokenizador correcto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp50xRT7FQPo",
        "outputId": "4abbd84a-ec71-4d2f-d0d7-12ff12bac4be"
      },
      "outputs": [],
      "source": [
        "#Ejemplo de tokenizaci√≥n\n",
        "sample_txt = 'hola mundo'\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "tokens_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfUYk1L9FTUG",
        "outputId": "3a6c4be8-2255-4a86-9dcb-04fd1f15bc06"
      },
      "outputs": [],
      "source": [
        "#Codificaci√≥n\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length = 10,\n",
        "    add_special_tokens = True, # Agrega [CLS] y [SEP]\n",
        "    return_token_type_ids = False,\n",
        "    padding='max_length',\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQLcb1yFW29",
        "outputId": "6aa56589-63a3-42eb-a9a2-16c25ee19b7c"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
        "print(encoding['input_ids'][0])\n",
        "print(encoding['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTsjguxqFaCC"
      },
      "outputs": [],
      "source": [
        "#Crear el dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self,comentarios,sentimiento,tokenizer,max_len):\n",
        "        self.comentarios = comentarios\n",
        "        self.sentimiento = sentimiento\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comentarios)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        comentario = str(self.comentarios[item])\n",
        "        label = self.sentimiento[item]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            comentario,\n",
        "            max_length = self.max_len,\n",
        "            add_special_tokens = True, # Agrega [CLS] y [SEP]\n",
        "            return_token_type_ids = False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt',\n",
        "            truncation= True\n",
        "        )\n",
        "        return {\n",
        "            'review': comentario,\n",
        "            'input_ids':encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a18TtH6dFcp6"
      },
      "outputs": [],
      "source": [
        "#Data Loader\n",
        "\n",
        "def data_loader(df,tokenizer,max_len,batch_size):\n",
        "    dataset = IMDBDataset(\n",
        "        comentarios=df.comentario.to_numpy(),\n",
        "        sentimiento = df.sentimiento.to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "    #Definir Samplers para clases desbalanceadas\n",
        "\n",
        "    #Calcular pesos inversos a la frecuencia de cada clase\n",
        "    class_counts = df['sentimiento'].value_counts().sort_index().values\n",
        "    class_weights = 1/np.array(class_counts)\n",
        "    sample_weights = class_weights[df['sentimiento'].values]\n",
        "\n",
        "    #Crear sampler ponderado\n",
        "    sampler = WeightedRandomSampler(sample_weights,num_samples=len(sample_weights),replacement = True)\n",
        "    return DataLoader(dataset,batch_size=BATCH_SIZE,num_workers=2,sampler = sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8lpCQ1jFgrb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df,test_size=0.2,random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test,tokenizer,MAX_LEN,BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhkcM6SUzWbD"
      },
      "outputs": [],
      "source": [
        "#Early Stopping por val_loss (No usado en la versi√≥n final)\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience, min_delta, path=\"best_model.pth\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): N√∫mero de √©pocas sin mejora antes de detener el entrenamiento.\n",
        "            min_delta (float): Cambio m√≠nimo en `val_loss` para considerar una mejora.\n",
        "            path (str): Ruta donde se guardar√° el mejor modelo.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss, model, fold):\n",
        "        \"\"\"Verifica si se debe detener el entrenamiento.\"\"\"\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            best_model_path = f\"/content/drive/My Drive/Intelektubies/Modelos/RoBERTuito_folds/V5/model_fold_{fold}.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)  # Guarda el mejor modelo\n",
        "            print(f\"üîπ Mejor modelo guardado en {best_model_path} (Val Loss: {val_loss:.5f})\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"‚ö†Ô∏è No hay mejora en {self.counter}/{self.patience} √©pocas.\")\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"‚èπÔ∏è Early Stopping activado. Deteniendo el entrenamiento.\")\n",
        "            return True  # Se detiene el entrenamiento\n",
        "\n",
        "        return False  # Contin√∫a el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h737otK7l0up"
      },
      "outputs": [],
      "source": [
        "class EarlyStoppingF1:\n",
        "    def __init__(self, patience=5, min_delta=0.001, path=\"best_model.pth\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.best_f1 = 0.0\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, current_f1, model, fold):\n",
        "        # Buscamos un incremento significativo en macro F1\n",
        "        if current_f1 > self.best_f1 + self.min_delta:\n",
        "            self.best_f1 = current_f1\n",
        "            self.counter = 0\n",
        "            best_model_path = f\"/content/drive/My Drive/Intelektubies/Modelos/RoBERTuito_folds/Early/model_fold_{fold}.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"üîπ Mejor modelo guardado en {best_model_path} (Macro F1: {current_f1:.4f})\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"‚ö†Ô∏è No hay mejora en {self.counter}/{self.patience} √©pocas (Macro F1: {current_f1:.4f}).\")\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"‚èπÔ∏è Early Stopping activado. Deteniendo el entrenamiento.\")\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8Bo8cFWFl40"
      },
      "outputs": [],
      "source": [
        "#MODELO\n",
        "from transformers import RobertaModel, AutoModel\n",
        "import torch.nn as nn\n",
        "class RoBERTtuitoSentimentClassifier(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super(RoBERTtuitoSentimentClassifier,self).__init__()\n",
        "        #self.roberta = RobertaModel.from_pretrained(PRE_TRAINED_MODEL)\n",
        "        self.roberta = AutoModel.from_pretrained(PRE_TRAINED_MODEL,add_pooling_layer=False)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.linear = nn.Linear(self.roberta.config.hidden_size,n_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # RoBERTa doesn't use pooler_output like BERT\n",
        "        # Use the first token's hidden state from the last_hidden_state\n",
        "        cls_output = output['last_hidden_state'][:, 0, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        drop_output = self.drop(cls_output)\n",
        "        output = self.linear(drop_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYdCMwYvFpNf"
      },
      "outputs": [],
      "source": [
        "model = RoBERTtuitoSentimentClassifier(NCLASES)\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MlIJicFtwU"
      },
      "outputs": [],
      "source": [
        "#Parametros del entrenamiento\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 7\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,weight_decay=0.001)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps= total_steps\n",
        ")\n",
        "#loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device=device)\n",
        "class_counts = df['sentimiento'].value_counts().sort_index().values\n",
        "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1).to(device)\n",
        "#early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA) - No se usa en la versi√≥n final por no mostrar un buen rendimiento en validaci√≥n\n",
        "early_stoppingf1 = EarlyStoppingF1(patience=7, min_delta=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9_KNJAFulq"
      },
      "outputs": [],
      "source": [
        "#Definici√≥n de Entrenamiento\n",
        "def train_model(model,data_loader,loss_fn,optimazer,device,scheduler,n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        _,preds = torch.max(outputs,dim=1)\n",
        "        loss = loss_fn(outputs,labels)\n",
        "        correct_predictions += torch.sum(preds==labels)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0) #Evita que el entrenamiento se estanque\n",
        "        optimazer.step()\n",
        "        scheduler.step()\n",
        "        optimazer.zero_grad() # reset\n",
        "    return correct_predictions.double()/n_examples,np.mean(losses)\n",
        "\n",
        "def eval_model(model,data_loader,loss_fn,device,n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_preds = 0\n",
        "    with torch.no_grad(): #No modificar ningun parametro\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "            _,preds = torch.max(outputs,dim=1)\n",
        "            loss = loss_fn(outputs,labels)\n",
        "            correct_preds += torch.sum(preds==labels)\n",
        "            losses.append(loss.item())\n",
        "    return correct_preds.double()/n_examples,np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm6h6z6muZET"
      },
      "outputs": [],
      "source": [
        "\n",
        "target_names = ['class Negativo', 'class Neutral', 'class Positivo', 'class Alerta']\n",
        "def eval_model_with_metrics(model, data_loader, loss_fn, device, n_examples, plot_confusion=False):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_preds = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():  # No modificar ningun parametro\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_preds.double() / n_examples\n",
        "    avg_loss = np.mean(losses)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    # M√©tricas de evaluaci√≥n usando scikit-learn\n",
        "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds,target_names=target_names))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "    print(\"Accuracy:\", accuracy.item())\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "\n",
        "    if plot_confusion:\n",
        "        # Visualizar la matriz de confusi√≥n solo cuando se indique\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    return accuracy, avg_loss, macro_f1, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpJv_biDudtg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def eval_model_with_auc(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    all_labels = []\n",
        "    all_preds_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Salida de logits y c√°lculo de probabilidades\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds_probs.extend(probs.cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    # Convierte all_preds_probs a un arreglo de NumPy\n",
        "    all_preds_probs = np.array(all_preds_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcula el ROC-AUC para cada clase usando el enfoque One-vs-One (OvO)\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds_probs, multi_class='ovo')\n",
        "\n",
        "    # Calcula la curva Precision-Recall y el AUC para cada clase\n",
        "    n_classes = all_preds_probs.shape[1]\n",
        "    pr_auc_list = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        # Obtener los valores binarios para la clase actual\n",
        "        binarized_labels = (all_labels == i).astype(int)\n",
        "        precision, recall, _ = precision_recall_curve(binarized_labels, all_preds_probs[:, i])\n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_list.append(pr_auc)\n",
        "\n",
        "    avg_pr_auc = np.mean(pr_auc_list)\n",
        "\n",
        "    print(f'ROC AUC Score: {roc_auc}')\n",
        "    print(f'Average Precision-Recall AUC: {avg_pr_auc}')\n",
        "\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUb5vYJmuj0i",
        "outputId": "6e957ebe-5e7a-4832-d304-ab0579d0130b"
      },
      "outputs": [],
      "source": [
        "# Inicializar variables para almacenar las m√©tricas por √©poca\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "    print(len(train_data_loader))\n",
        "    train_acc, train_loss = train_model(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(df_train)\n",
        "    )\n",
        "\n",
        "    val_acc, val_loss, macro_f1, test_labels, test_preds = eval_model_with_metrics(\n",
        "        model,\n",
        "        test_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(df_test),\n",
        "        plot_confusion=False  # No graficar durante las √©pocas\n",
        "    )\n",
        "        # Almacenar las m√©tricas\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc.item())  # Convertir a float si es tensor\n",
        "    test_accuracies.append(val_acc.item())  # Convertir a float si es tensor\n",
        "\n",
        "    print(f\"üìâ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"üìà Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    print()\n",
        "    # Verificar si hay que detener el entrenamiento\n",
        "    if early_stoppingf1(macro_f1, model, 5):\n",
        "      break  # Se interrumpe el entrenamiento si no hay mejora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9anhLvQJxgS"
      },
      "outputs": [],
      "source": [
        "def classifySentiment(review_text):\n",
        "    encoding_review = tokenizer.encode_plus(\n",
        "        review_text,  #\n",
        "        max_length=10,\n",
        "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "\n",
        "    )\n",
        "\n",
        "    input_ids = encoding_review['input_ids'].to(device)\n",
        "    attention_mask = encoding_review['attention_mask'].to(device)\n",
        "    output = model(input_ids, attention_mask)\n",
        "    _, prediction = torch.max(output, dim=1)\n",
        "\n",
        "    # Mapeo\n",
        "    sentiment_mapping = {0: 'Negativo', 1: 'Neutral', 2: 'Positivo',3 : 'Alerta'}\n",
        "\n",
        "\n",
        "    predicted_sentiment = sentiment_mapping[prediction.item()]\n",
        "    print(f\"Texto: {review_text}\")\n",
        "    print(f\"Sentimiento predicho: {predicted_sentiment}\")\n",
        "\n",
        "    return predicted_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7MR8juTKIhu",
        "outputId": "ec83966e-1dbe-4542-c96f-cba7c93a6ca1"
      },
      "outputs": [],
      "source": [
        "sample_txt = 'Debe mejorar: Muy buen profesor, pero deber√≠a ser m√°s exigente porque cualquiera puede pasar'\n",
        "\n",
        "print(classifySentiment(sample_txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IctKomPmKQ-l",
        "outputId": "5aa4ec5e-d803-4ab0-e010-dfc1817ac55d"
      },
      "outputs": [],
      "source": [
        "sample_list = ['Tiene una actitud sumamente correcta que se irradia a los estudiantes',\n",
        "  'Su conocimiento',\n",
        "  'respeta los horarios de clases',\n",
        "  'nada, no sabe ense√±ar y no sabe la materia practica',\n",
        "  'Excelente docente, nos guio para poder sobresalir siempre en su materia y nos enseno del futuro de un ingeniero.',\n",
        "  'es bien organizada y exigente',\n",
        "  'Buen dominio en la MATERIA',\n",
        "  'nada, no sabe ense√±ar y no sabe la materia practica',\n",
        "  'El inter√©s por fomentar el trabajo en equipo',\n",
        "  'Buena ense√±anza',\n",
        "  'conocimientos  de la asignatura',\n",
        "  'Desarrollo de la materia  con claridad',\n",
        "  'SU CONOCIMIENTO DE LA ASIGNATURA',\n",
        "  'Su compromiso',\n",
        "  'Su puntualidad',\n",
        "  'la puntualidad',\n",
        "  '-Capacidad de aprendizaje. -Conocimiento de los temas. -CAPACIDAD PARA PROPONER ACTIVIDADES RELACIONADAS CON CADA UNO DE LOS TEMAS DICTADOS EN CLASE.',\n",
        "  'Conceptos generales',\n",
        "  'mucho conocimiento de la asignatura',\n",
        "  'SU compresion de la materia']\n",
        "\n",
        "\n",
        "for sample in sample_list:\n",
        "  print(classifySentiment(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztqYNoEf_jXL",
        "outputId": "ea0fcd98-5472-474c-958e-754e4aacf87e"
      },
      "outputs": [],
      "source": [
        "sample_txt = 'En una pr√°ctica, no respond√≠ una pregunta y la docente asumi√≥ que no asist√≠, d√°ndome cero en el informe.luego en clase, mencion√≥ el incidente avergonz√°ndome frente a todos y no solo ami sino a dos estudiantes mas.no deberia hacer eso'\n",
        "print(classifySentiment(sample_txt))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
