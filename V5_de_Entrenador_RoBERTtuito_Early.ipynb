{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkHrU7a18bCN"
      },
      "source": [
        "# Entrenamiento K-Folds con parada temprana\n",
        "\n",
        "Autores:\n",
        "- Pablo Quito\n",
        "- Juan Valdiviezo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk6NLgLj8WzL",
        "outputId": "be0dc3a6-79be-48e6-fd79-e9c3a6c172c9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki9Haluf8hlN"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "from textwrap import wrap\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import RobertaModel, AutoModel\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PkgGY3Y8vyd",
        "outputId": "80141ae5-f255-4d65-a63c-afadf5a8cdb6"
      },
      "outputs": [],
      "source": [
        "# Inicialización\n",
        "ROUTE = '/content/drive/My Drive/Intelektubies/Datos/Entrenamiento V5'\n",
        "FILE_NAME = 'df_entrenamiento_v5.xlsx'\n",
        "RANDOM_SEED = 25\n",
        "MAX_LEN = 130\n",
        "BATCH_SIZE = 16  #Anterior 8\n",
        "N_SPLITS = 5  # K-Folds\n",
        "DATASET_PATH = ROUTE + '/' + FILE_NAME\n",
        "NCLASES = 4  # Positivo, Negativo, Neutro, Alerta\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Fuente de datos: \" + DATASET_PATH)\n",
        "print(\"Dispositivo: \" + str(device))\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "OViKYibE_xBi",
        "outputId": "455d4102-0d7a-4dc3-e381-4a09a417cc67"
      },
      "outputs": [],
      "source": [
        "# Cargar datos\n",
        "df = pd.read_excel(DATASET_PATH)\n",
        "print(df.shape)\n",
        "print(\"\\n\".join(wrap(df['comentario'][666])))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "C34YQb7xEmNV",
        "outputId": "188de0de-6f06-4742-d692-465071d3be75"
      },
      "outputs": [],
      "source": [
        "#Mapeo a las categorias para el modelo\n",
        "df['sentimiento'] = df['sentimiento'].map({'Positiva': 2, 'Negativa': 0, 'Neutral': 1,'Alerta': 3}).astype(int)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6vLulF_FJ7Y",
        "outputId": "63843cde-4f31-450c-eefe-0914e8eb04c4"
      },
      "outputs": [],
      "source": [
        "# TOKENIZACIÓN\n",
        "PRE_TRAINED_MODEL = 'pysentimiento/robertuito-base-uncased-emotion'\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)  # Usa AutoTokenizer para elegir el tokenizador correcto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp50xRT7FQPo",
        "outputId": "4abbd84a-ec71-4d2f-d0d7-12ff12bac4be"
      },
      "outputs": [],
      "source": [
        "#Ejemplo de tokenización\n",
        "sample_txt = 'hola mundo'\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "tokens_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfUYk1L9FTUG",
        "outputId": "3a6c4be8-2255-4a86-9dcb-04fd1f15bc06"
      },
      "outputs": [],
      "source": [
        "#Codificación\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length = 10,\n",
        "    add_special_tokens = True, # Agrega [CLS] y [SEP]\n",
        "    return_token_type_ids = False,\n",
        "    padding='max_length',\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQLcb1yFW29",
        "outputId": "6aa56589-63a3-42eb-a9a2-16c25ee19b7c"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
        "print(encoding['input_ids'][0])\n",
        "print(encoding['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTsjguxqFaCC"
      },
      "outputs": [],
      "source": [
        "#Crear el dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self,comentarios,sentimiento,tokenizer,max_len):\n",
        "        self.comentarios = comentarios\n",
        "        self.sentimiento = sentimiento\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comentarios)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        comentario = str(self.comentarios[item])\n",
        "        label = self.sentimiento[item]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            comentario,\n",
        "            max_length = self.max_len,\n",
        "            add_special_tokens = True, # Agrega [CLS] y [SEP]\n",
        "            return_token_type_ids = False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt',\n",
        "            truncation= True\n",
        "        )\n",
        "        return {\n",
        "            'review': comentario,\n",
        "            'input_ids':encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a18TtH6dFcp6"
      },
      "outputs": [],
      "source": [
        "#Data Loader\n",
        "\n",
        "def data_loader(df,tokenizer,max_len,batch_size):\n",
        "    dataset = IMDBDataset(\n",
        "        comentarios=df.comentario.to_numpy(),\n",
        "        sentimiento = df.sentimiento.to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "    #Definir Samplers para clases desbalanceadas\n",
        "\n",
        "    #Calcular pesos inversos a la frecuencia de cada clase\n",
        "    class_counts = df['sentimiento'].value_counts().sort_index().values\n",
        "    class_weights = 1/np.array(class_counts)\n",
        "    sample_weights = class_weights[df['sentimiento'].values]\n",
        "\n",
        "    #Crear sampler ponderado\n",
        "    sampler = WeightedRandomSampler(sample_weights,num_samples=len(sample_weights),replacement = True)\n",
        "    return DataLoader(dataset,batch_size=BATCH_SIZE,num_workers=2,sampler = sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8lpCQ1jFgrb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df,test_size=0.2,random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test,tokenizer,MAX_LEN,BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhkcM6SUzWbD"
      },
      "outputs": [],
      "source": [
        "#Early Stopping por val_loss (No usado en la versión final)\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience, min_delta, path=\"best_model.pth\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): Número de épocas sin mejora antes de detener el entrenamiento.\n",
        "            min_delta (float): Cambio mínimo en `val_loss` para considerar una mejora.\n",
        "            path (str): Ruta donde se guardará el mejor modelo.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss, model, fold):\n",
        "        \"\"\"Verifica si se debe detener el entrenamiento.\"\"\"\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            best_model_path = f\"/content/drive/My Drive/Intelektubies/Modelos/RoBERTuito_folds/V5/model_fold_{fold}.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)  # Guarda el mejor modelo\n",
        "            print(f\"🔹 Mejor modelo guardado en {best_model_path} (Val Loss: {val_loss:.5f})\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"⚠️ No hay mejora en {self.counter}/{self.patience} épocas.\")\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"⏹️ Early Stopping activado. Deteniendo el entrenamiento.\")\n",
        "            return True  # Se detiene el entrenamiento\n",
        "\n",
        "        return False  # Continúa el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h737otK7l0up"
      },
      "outputs": [],
      "source": [
        "class EarlyStoppingF1:\n",
        "    def __init__(self, patience=5, min_delta=0.001, path=\"best_model.pth\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.best_f1 = 0.0\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, current_f1, model, fold):\n",
        "        # Buscamos un incremento significativo en macro F1\n",
        "        if current_f1 > self.best_f1 + self.min_delta:\n",
        "            self.best_f1 = current_f1\n",
        "            self.counter = 0\n",
        "            best_model_path = f\"/content/drive/My Drive/Intelektubies/Modelos/RoBERTuito_folds/Early/model_fold_{fold}.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"🔹 Mejor modelo guardado en {best_model_path} (Macro F1: {current_f1:.4f})\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"⚠️ No hay mejora en {self.counter}/{self.patience} épocas (Macro F1: {current_f1:.4f}).\")\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"⏹️ Early Stopping activado. Deteniendo el entrenamiento.\")\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8Bo8cFWFl40"
      },
      "outputs": [],
      "source": [
        "#MODELO\n",
        "from transformers import RobertaModel, AutoModel\n",
        "import torch.nn as nn\n",
        "class RoBERTtuitoSentimentClassifier(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super(RoBERTtuitoSentimentClassifier,self).__init__()\n",
        "        #self.roberta = RobertaModel.from_pretrained(PRE_TRAINED_MODEL)\n",
        "        self.roberta = AutoModel.from_pretrained(PRE_TRAINED_MODEL,add_pooling_layer=False)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.linear = nn.Linear(self.roberta.config.hidden_size,n_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # RoBERTa doesn't use pooler_output like BERT\n",
        "        # Use the first token's hidden state from the last_hidden_state\n",
        "        cls_output = output['last_hidden_state'][:, 0, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        drop_output = self.drop(cls_output)\n",
        "        output = self.linear(drop_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYdCMwYvFpNf"
      },
      "outputs": [],
      "source": [
        "model = RoBERTtuitoSentimentClassifier(NCLASES)\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MlIJicFtwU"
      },
      "outputs": [],
      "source": [
        "#Parametros del entrenamiento\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 7\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,weight_decay=0.001)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps= total_steps\n",
        ")\n",
        "#loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device=device)\n",
        "class_counts = df['sentimiento'].value_counts().sort_index().values\n",
        "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1).to(device)\n",
        "#early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA) - No se usa en la versión final por no mostrar un buen rendimiento en validación\n",
        "early_stoppingf1 = EarlyStoppingF1(patience=7, min_delta=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9_KNJAFulq"
      },
      "outputs": [],
      "source": [
        "#Definición de Entrenamiento\n",
        "def train_model(model,data_loader,loss_fn,optimazer,device,scheduler,n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        _,preds = torch.max(outputs,dim=1)\n",
        "        loss = loss_fn(outputs,labels)\n",
        "        correct_predictions += torch.sum(preds==labels)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0) #Evita que el entrenamiento se estanque\n",
        "        optimazer.step()\n",
        "        scheduler.step()\n",
        "        optimazer.zero_grad() # reset\n",
        "    return correct_predictions.double()/n_examples,np.mean(losses)\n",
        "\n",
        "def eval_model(model,data_loader,loss_fn,device,n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_preds = 0\n",
        "    with torch.no_grad(): #No modificar ningun parametro\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "            _,preds = torch.max(outputs,dim=1)\n",
        "            loss = loss_fn(outputs,labels)\n",
        "            correct_preds += torch.sum(preds==labels)\n",
        "            losses.append(loss.item())\n",
        "    return correct_preds.double()/n_examples,np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm6h6z6muZET"
      },
      "outputs": [],
      "source": [
        "\n",
        "target_names = ['class Negativo', 'class Neutral', 'class Positivo', 'class Alerta']\n",
        "def eval_model_with_metrics(model, data_loader, loss_fn, device, n_examples, plot_confusion=False):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_preds = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():  # No modificar ningun parametro\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_preds.double() / n_examples\n",
        "    avg_loss = np.mean(losses)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    # Métricas de evaluación usando scikit-learn\n",
        "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds,target_names=target_names))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "    print(\"Accuracy:\", accuracy.item())\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "\n",
        "    if plot_confusion:\n",
        "        # Visualizar la matriz de confusión solo cuando se indique\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    return accuracy, avg_loss, macro_f1, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpJv_biDudtg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def eval_model_with_auc(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    all_labels = []\n",
        "    all_preds_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Salida de logits y cálculo de probabilidades\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds_probs.extend(probs.cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    # Convierte all_preds_probs a un arreglo de NumPy\n",
        "    all_preds_probs = np.array(all_preds_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcula el ROC-AUC para cada clase usando el enfoque One-vs-One (OvO)\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds_probs, multi_class='ovo')\n",
        "\n",
        "    # Calcula la curva Precision-Recall y el AUC para cada clase\n",
        "    n_classes = all_preds_probs.shape[1]\n",
        "    pr_auc_list = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        # Obtener los valores binarios para la clase actual\n",
        "        binarized_labels = (all_labels == i).astype(int)\n",
        "        precision, recall, _ = precision_recall_curve(binarized_labels, all_preds_probs[:, i])\n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_list.append(pr_auc)\n",
        "\n",
        "    avg_pr_auc = np.mean(pr_auc_list)\n",
        "\n",
        "    print(f'ROC AUC Score: {roc_auc}')\n",
        "    print(f'Average Precision-Recall AUC: {avg_pr_auc}')\n",
        "\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUb5vYJmuj0i",
        "outputId": "6e957ebe-5e7a-4832-d304-ab0579d0130b"
      },
      "outputs": [],
      "source": [
        "# Inicializar variables para almacenar las métricas por época\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "    print(len(train_data_loader))\n",
        "    train_acc, train_loss = train_model(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(df_train)\n",
        "    )\n",
        "\n",
        "    val_acc, val_loss, macro_f1, test_labels, test_preds = eval_model_with_metrics(\n",
        "        model,\n",
        "        test_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(df_test),\n",
        "        plot_confusion=False  # No graficar durante las épocas\n",
        "    )\n",
        "        # Almacenar las métricas\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc.item())  # Convertir a float si es tensor\n",
        "    test_accuracies.append(val_acc.item())  # Convertir a float si es tensor\n",
        "\n",
        "    print(f\"📉 Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"📈 Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    print()\n",
        "    # Verificar si hay que detener el entrenamiento\n",
        "    if early_stoppingf1(macro_f1, model, 5):\n",
        "      break  # Se interrumpe el entrenamiento si no hay mejora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9anhLvQJxgS"
      },
      "outputs": [],
      "source": [
        "def classifySentiment(review_text):\n",
        "    encoding_review = tokenizer.encode_plus(\n",
        "        review_text,  #\n",
        "        max_length=10,\n",
        "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "\n",
        "    )\n",
        "\n",
        "    input_ids = encoding_review['input_ids'].to(device)\n",
        "    attention_mask = encoding_review['attention_mask'].to(device)\n",
        "    output = model(input_ids, attention_mask)\n",
        "    _, prediction = torch.max(output, dim=1)\n",
        "\n",
        "    # Mapeo\n",
        "    sentiment_mapping = {0: 'Negativo', 1: 'Neutral', 2: 'Positivo',3 : 'Alerta'}\n",
        "\n",
        "\n",
        "    predicted_sentiment = sentiment_mapping[prediction.item()]\n",
        "    print(f\"Texto: {review_text}\")\n",
        "    print(f\"Sentimiento predicho: {predicted_sentiment}\")\n",
        "\n",
        "    return predicted_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7MR8juTKIhu",
        "outputId": "ec83966e-1dbe-4542-c96f-cba7c93a6ca1"
      },
      "outputs": [],
      "source": [
        "sample_txt = 'Debe mejorar: Muy buen profesor, pero debería ser más exigente porque cualquiera puede pasar'\n",
        "\n",
        "print(classifySentiment(sample_txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IctKomPmKQ-l",
        "outputId": "5aa4ec5e-d803-4ab0-e010-dfc1817ac55d"
      },
      "outputs": [],
      "source": [
        "sample_list = ['Tiene una actitud sumamente correcta que se irradia a los estudiantes',\n",
        "  'Su conocimiento',\n",
        "  'respeta los horarios de clases',\n",
        "  'nada, no sabe enseñar y no sabe la materia practica',\n",
        "  'Excelente docente, nos guio para poder sobresalir siempre en su materia y nos enseno del futuro de un ingeniero.',\n",
        "  'es bien organizada y exigente',\n",
        "  'Buen dominio en la MATERIA',\n",
        "  'nada, no sabe enseñar y no sabe la materia practica',\n",
        "  'El interés por fomentar el trabajo en equipo',\n",
        "  'Buena enseñanza',\n",
        "  'conocimientos  de la asignatura',\n",
        "  'Desarrollo de la materia  con claridad',\n",
        "  'SU CONOCIMIENTO DE LA ASIGNATURA',\n",
        "  'Su compromiso',\n",
        "  'Su puntualidad',\n",
        "  'la puntualidad',\n",
        "  '-Capacidad de aprendizaje. -Conocimiento de los temas. -CAPACIDAD PARA PROPONER ACTIVIDADES RELACIONADAS CON CADA UNO DE LOS TEMAS DICTADOS EN CLASE.',\n",
        "  'Conceptos generales',\n",
        "  'mucho conocimiento de la asignatura',\n",
        "  'SU compresion de la materia']\n",
        "\n",
        "\n",
        "for sample in sample_list:\n",
        "  print(classifySentiment(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztqYNoEf_jXL",
        "outputId": "ea0fcd98-5472-474c-958e-754e4aacf87e"
      },
      "outputs": [],
      "source": [
        "sample_txt = 'En una práctica, no respondí una pregunta y la docente asumió que no asistí, dándome cero en el informe.luego en clase, mencionó el incidente avergonzándome frente a todos y no solo ami sino a dos estudiantes mas.no deberia hacer eso'\n",
        "print(classifySentiment(sample_txt))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
